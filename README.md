# covid_19_ML_Algo
Descriptive Research on Covid-19 Dataset


## Introduction

This repository hosts a machine learning project aimed at analyzing and predicting trends related to the COVID-19 pandemic. Leveraging machine learning algorithms, we seek to gain insights into the spread of the virus, predict future trends, and assist in decision-making processes related to public health interventions.



## Dataset: 
The project utilizes publicly available datasets related to COVID-19, including data on confirmed cases, deaths, recoveries, testing rates, vaccination rates, and other relevant metrics. The datasets are sourced from reputable sources such as the World Health Organization (WHO), Centers for Disease Control and Prevention (CDC), and other official health organizations.


## Tools and Technologies
The project is developed using Python programming language and various libraries and frameworks commonly used in machine learning and data analysis, including but not limited to:


NumPy
Pandas
Scikit-learn
TensorFlow
Matplotlib
Seaborn
Project Structure


# The repository is organized as follows:

## dataset: 

Contains the datasets used in the project.

## Size of dataset: 199999 Rows & 20 Variables

## notebooks: 

Jupyter notebooks illustrating the data analysis, preprocessing, model training, and evaluation steps.

## src: 

Python scripts containing reusable code modules for data preprocessing, feature engineering, model training, and evaluation.

## models: 

Saved machine learning models trained on the COVID-19 datasets.

## Results: 

Contains visualizations, reports, and summaries of the project findings.

## requirements.txt: 

Specifies the Python dependencies required to run the project.


## Usage

To run the project locally, follow these steps:

Clone the repository to your local machine.
Navigate to the project directory.
Install the required dependencies by running:

Copy code
pip install -r requirements.txt


Explore the Jupyter notebooks in the notebooks/ directory to understand the data analysis and model development process.
Experiment with the provided scripts in the src/ directory to preprocess data, train models, and evaluate their performance.
Use the trained models in the models/ directory to make predictions or generate insights.
Refer to the documentation and comments within the code for detailed explanations of each step.

# Steps involved
### 1. EDA

### 2. Feature selection

### 3. Various Statistical Analysis

### 4. Hypothesis Testing

### 5. Selecting Appropriate ML Algorithm to predict the selected target variable

### 6. Building Model

### 7. Training

### 8. Testing & Evaluation

### 9. Conclusion

Acknowledgments
We would like to express our gratitude to the organizations and individuals who have collected and shared the data used in this project. Additionally, we acknowledge the contributions of the open-source community, whose libraries and frameworks have made this project possible.

